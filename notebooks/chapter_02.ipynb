{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXU3RthDTNl-"
   },
   "source": [
    "# Chapter 2\n",
    "\n",
    "## Look at the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUMoZEhbSlJQ"
   },
   "outputs": [],
   "source": [
    "# Listing Load the data from the csvâ€files\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-8KsOHCTSZY"
   },
   "outputs": [],
   "source": [
    "# Listing The shapes of the Titanic datasets\n",
    "print('train has {} rows and {} columns'.format(*train.shape))\n",
    "print('test has {} rows and {} columns'.format(*test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNZSnVxaTVx2"
   },
   "outputs": [],
   "source": [
    "# Listing The structure of the train dataset\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1kZilFqTX_-"
   },
   "outputs": [],
   "source": [
    "# Listing The structure of the test dataset\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-lekRmWTaEl"
   },
   "outputs": [],
   "source": [
    "# Listing Look at the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcxxNapoTf8U"
   },
   "source": [
    "## Section Data Preparation and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brc7a_rZTcRG"
   },
   "outputs": [],
   "source": [
    "# Listing Cope with missing values\n",
    "# option 1\n",
    "# We only have two passengers without it. This is bearable\n",
    "train = train.dropna(subset=[\"Embarked\"])    \n",
    "\n",
    "# option 2\n",
    "# We only have very few information about the cabin, let's drop it\n",
    "train = train.drop(\"Cabin\", axis=1)       \n",
    "\n",
    "# option 3\n",
    "# The age misses quite a few times. But intuition\n",
    "# says it might be important for someone's chance to survive.\n",
    "mean = train[\"Age\"].mean()\n",
    "train[\"Age\"] = train[\"Age\"].fillna(mean)     \n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smE5unV3TlIW"
   },
   "outputs": [],
   "source": [
    "# Listing Unique values in columns\n",
    "print('There are {} different (unique) PassengerIds in the data'\n",
    "    .format(train[\"PassengerId\"].nunique()))\n",
    "print('There are {} different (unique) names in the data'\n",
    "    .format(train[\"Name\"].nunique()))\n",
    "print('There are {} different (unique) ticket numbers in the data'\n",
    "    .format(train[\"Ticket\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSqjnE1PTpXM"
   },
   "outputs": [],
   "source": [
    "# Listing Remove identifying data\n",
    "train = train.drop(\"PassengerId\", axis=1)\n",
    "train = train.drop(\"Name\", axis=1)\n",
    "train = train.drop(\"Ticket\", axis=1)\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOp_MrIsTtyy"
   },
   "outputs": [],
   "source": [
    "# Listing Transforming textual data into numbers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in ['Sex', 'Embarked']:\n",
    "    le.fit(train[col])\n",
    "    train[col] = le.transform(train[col])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLxgWDs7TukS"
   },
   "outputs": [],
   "source": [
    "# Listing The maximum values\n",
    "print('The maximum age is {}'.format(train[\"Age\"].max()))\n",
    "print('The maximum fare is {}'.format(train[\"Fare\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5s17xDjhTw2f"
   },
   "outputs": [],
   "source": [
    "# Listing Normalization of the data.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "\n",
    "print('The minimum value is {} and the maximum value is {}'\n",
    "    .format(train.min(), train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4L9jrtrTzFQ"
   },
   "outputs": [],
   "source": [
    "# Listing Separating input from labels and training from testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_data = train[:, 1:8]\n",
    "labels = train[:, 0]\n",
    "\n",
    "train_input, test_input, train_labels, test_labels = train_test_split(\n",
    "    input_data, labels, test_size = 0.2)\n",
    "\n",
    "print('We have {} training and {} testing rows'.format(train_input.shape[0], test_input.shape[0]))\n",
    "print('There are {} input columns'.format(train_input.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phq3UrfFT1JQ"
   },
   "outputs": [],
   "source": [
    "# Listing Save the data to the filesystem\n",
    "import numpy as np\n",
    "\n",
    "with open('train.npy', 'wb') as f:\n",
    "    np.save(f, train_input)\n",
    "    np.save(f, train_labels)\n",
    "\n",
    "with open('test.npy', 'wb') as f:\n",
    "    np.save(f, test_input)\n",
    "    np.save(f, test_labels)\n",
    "#CAPTION Save the data to the filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAHDY0sfT3cG"
   },
   "source": [
    "## Section Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCGsn4YjT5pT"
   },
   "outputs": [],
   "source": [
    "# Listing A random classifier\n",
    "import random\n",
    "random.seed(a=None, version=2)\n",
    " \n",
    "def classify(passenger):\n",
    "    return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml0VzYHbT_DE"
   },
   "outputs": [],
   "source": [
    "# Listing The classification runner\n",
    "def run(f_classify, x):\n",
    "    return list(map(f_classify, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2rljZ88UDHw"
   },
   "outputs": [],
   "source": [
    "# Listing Run the classifier\n",
    "result = run(classify, train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqHOnjOaUHVx"
   },
   "outputs": [],
   "source": [
    "# Listing Evaluate the classifier\n",
    "def evaluate(predictions, actual):\n",
    "    correct = list(filter(\n",
    "        lambda item: item[0] == item[1],\n",
    "        list(zip(predictions,actual))\n",
    "    ))\n",
    "    return '{} correct predictions out of {}. Accuracy {:.0f} %' \\\n",
    "        .format(len(correct), len(actual), 100*len(correct)/len(actual))\n",
    "\n",
    "print(evaluate(run(classify, train_input), train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-y2JY5lUIGi"
   },
   "outputs": [],
   "source": [
    "# Listing Always predict a passenger died\n",
    "def predict_death(item):\n",
    "    return 0\n",
    "\n",
    "print(evaluate(run(predict_death, train_input), train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuJfVszKUNxJ"
   },
   "source": [
    "## Section Classifier Evaluation and Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZuPirhDUKi-"
   },
   "outputs": [],
   "source": [
    "# Listing Confustion matrix of the predict death classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = run(predict_death, train_input)\n",
    "confusion_matrix(train_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJqjVeqZUUPM"
   },
   "outputs": [],
   "source": [
    "# Listing The precision score\n",
    "from sklearn.metrics import precision_score\n",
    "print('The precision score of the predict_death classifier is {}'\n",
    "    .format(precision_score(train_labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhVcWDl7UWd9"
   },
   "outputs": [],
   "source": [
    "# Listing The recall score\n",
    "from sklearn.metrics import recall_score\n",
    "print('The recall score of the predict_death classifier is {}'\n",
    "    .format(recall_score(train_labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA-cXz_zUYtN"
   },
   "outputs": [],
   "source": [
    "# Listing The specificity and the npv\n",
    "def specificity(matrix):\n",
    "    return matrix[0][0]/(matrix[0][0]+matrix[0][1]) if (matrix[0][0]+matrix[0][1] > 0) else 0\n",
    "\n",
    "def npv(matrix):\n",
    "    return matrix[0][0]/(matrix[0][0]+matrix[1][0]) if (matrix[0][0]+matrix[1][0] > 0) else 0\n",
    "\n",
    "cm = confusion_matrix(train_labels, predictions)\n",
    "\n",
    "print('The specificity score of the predict_death classifier is {:.2f}'.format(specificity(cm)))\n",
    "print('The npv score of the predict_death classifier is {:.2f}'.format(npv(cm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRp8cfchUbG6"
   },
   "outputs": [],
   "source": [
    "# Listing The scores of the random classifier\n",
    "random_predictions = run(classify, train_input)\n",
    "random_cm = confusion_matrix(train_labels, random_predictions)\n",
    "\n",
    "print('The precision score of the random classifier is {:.2f}'\n",
    "    .format(precision_score(train_labels, random_predictions)))\n",
    "print('The recall score of the random classifier is {:.2f}'\n",
    "    .format(recall_score(train_labels, random_predictions)))\n",
    "print('The specificity score of the random classifier is {:.2f}'\n",
    "    .format(specificity(random_cm)))\n",
    "print('The npv score of the random classifier is {:.2f}'\n",
    "    .format(npv(random_cm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB0c3-AGUhG_"
   },
   "source": [
    "## Section Unmask the Hypocrite Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl-iDyPqUhwX"
   },
   "outputs": [],
   "source": [
    "# Listing A hypocrite classifier\n",
    "def hypocrite(passenger, weight):\n",
    "    return round(min(1,max(0,weight*0.5+random.uniform(0, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SepL4VSjUj7a"
   },
   "outputs": [],
   "source": [
    "# Listing The scores of the hypocrite classifier\n",
    "w_predictions = run(lambda passenger: hypocrite(passenger, -0.5), train_input)\n",
    "w_cm = confusion_matrix(train_labels, w_predictions)\n",
    "\n",
    "print('The precision score of the hypocrite classifier is {:.2f}'\n",
    "    .format(precision_score(train_labels, w_predictions)))\n",
    "print('The recall score of the hypocrite classifier is {:.2f}'\n",
    "    .format(recall_score(train_labels, w_predictions)))\n",
    "print('The specificity score of the hypocrite classifier is {:.2f}'\n",
    "    .format(specificity(w_cm)))\n",
    "print('The npv score of the hypocrite classifier is {:.2f}'\n",
    "    .format(npv(w_cm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGZlMOaWUmqn"
   },
   "outputs": [],
   "source": [
    "# Listing Run the hypocrite classifiers\n",
    "import numpy as np\n",
    "\n",
    "# number of steps to consider between -1 and 1\n",
    "cnt_steps = 40\n",
    "\n",
    "# a list of the step numbers [0, 1, ..., 38, 39]\n",
    "steps = np.arange(0, cnt_steps, 1).tolist()\n",
    "\n",
    "# list of the weights at every step [-1, -0.95, ... 0.9, 0.95, 1.0]\n",
    "weights = list(map(\n",
    "    lambda weight: round(weight, 2),\n",
    "    np.arange(-1, 1+2/(cnt_steps-1), 2/(cnt_steps-1)).tolist()\n",
    "))\n",
    "\n",
    "# list of predictions at every step\n",
    "l_predictions = list(map(\n",
    "    lambda step: run(\n",
    "        lambda passenger: hypocrite(passenger, weights[step]),\n",
    "        train_input\n",
    "    ),\n",
    "    steps\n",
    "))\n",
    "\n",
    "# list of confusion matrices at every steo\n",
    "l_cm = list(map(\n",
    "    lambda step: confusion_matrix(train_labels, l_predictions[step]),\n",
    "    steps\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJAoygrbUomb"
   },
   "outputs": [],
   "source": [
    "# Listing Plot the distribution of predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# create a graph for the number of predicted deaths\n",
    "deaths, = plt.plot(\n",
    "    weights, # point at x-axis\n",
    "    list(map(lambda cur: l_cm[cur][0][0]+l_cm[cur][1][0], steps)),\n",
    "    'lightsalmon', # color of the graph\n",
    "    label='Predicted death'\n",
    ")\n",
    "\n",
    "# create a graph for the number of predicted survivals\n",
    "survivals, = plt.plot(\n",
    "    weights, # point at x-axis\n",
    "    list(map(lambda cur: l_cm[cur][0][1]+l_cm[cur][1][1], steps)),\n",
    "    'lightgreen', # color of the graph\n",
    "    label='Predicted survival'\n",
    ")\n",
    "\n",
    "plt.legend(handles=[deaths, survivals],loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.15), framealpha=0.0, ncol=2)\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Number of predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAaVd5BzUqZj"
   },
   "outputs": [],
   "source": [
    "# Listing Metrics of the hypocrite classifier\n",
    "l_precision = list(map(lambda step: precision_score(train_labels, l_predictions[step]),steps))\n",
    "l_recall = list(map(lambda step: recall_score(train_labels, l_predictions[step]),steps))\n",
    "l_specificity = list(map(lambda step: specificity(l_cm[step]),steps))\n",
    "l_npv = list(map(lambda step: npv(l_cm[step]),steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVE6wrbaUuu1"
   },
   "source": [
    "In these four lines, we calculate the four metrics\n",
    "at each step. Let's visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfHXA0CtUsZR"
   },
   "outputs": [],
   "source": [
    "# Listing Plot the performance measures\n",
    "m_precision, = plt.plot(weights, l_precision, 'pink', label=\"precision\")\n",
    "m_recall, = plt.plot(weights, l_recall, 'cyan', label=\"recall\")\n",
    "m_specificity, = plt.plot(weights, l_specificity, 'gold', label=\"specificity\")\n",
    "m_npv, = plt.plot(weights, l_npv, 'coral', label=\"npv\")\n",
    "\n",
    "plt.legend(\n",
    "    handles=[m_precision, m_recall, m_specificity, m_npv],\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.15),\n",
    "    framealpha=0.0,\n",
    "    ncol=4)\n",
    "\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Number of predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zKhLVnUUy3e"
   },
   "outputs": [],
   "source": [
    "# Listing Calculating the mean of the measures\n",
    "l_mean = list(map(lambda step: sum(step)*0.25, zip(l_precision, l_recall, l_specificity, l_npv)))\n",
    "m_mean, = plt.plot(weights, l_mean, 'pink', label=\"Mean of the measures\")\n",
    "\n",
    "plt.legend(handles=[m_mean],loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.15),framealpha=0.0)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2jwcPUkU06e"
   },
   "outputs": [],
   "source": [
    "# Listing A reusable function to unmask the hypocrite classifier\n",
    "def classifier_report(name, run, classify, input, labels):\n",
    "    cr_predictions = run(classify, input)\n",
    "    cr_cm = confusion_matrix(labels, cr_predictions)\n",
    "\n",
    "    cr_precision = precision_score(labels, cr_predictions)\n",
    "    cr_recall = recall_score(labels, cr_predictions)\n",
    "    cr_specificity = specificity(cr_cm)\n",
    "    cr_npv = npv(cr_cm)\n",
    "    cr_level = 0.25*(cr_precision + cr_recall + cr_specificity + cr_npv)\n",
    "\n",
    "    print('The precision score of the {} classifier is {:.2f}'\n",
    "        .format(name, cr_precision))\n",
    "    print('The recall score of the {} classifier is {:.2f}'\n",
    "        .format(name, cr_recall))\n",
    "    print('The specificity score of the {} classifier is {:.2f}'\n",
    "        .format(name, cr_specificity))\n",
    "    print('The npv score of the {} classifier is {:.2f}'\n",
    "        .format(name, cr_npv))\n",
    "    print('The information level is: {:.2f}'\n",
    "        .format(cr_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aQSm_JXU2xW"
   },
   "outputs": [],
   "source": [
    "# Listing The report of the random classifier\n",
    "classifier_report(\n",
    "    \"Random PQC\", \n",
    "    run,\n",
    "    classify,\n",
    "    train_input,\n",
    "    train_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0xh3Lfza6+LoxFrW0iaDC",
   "mount_file_id": "1FXZQl9YIJ80Z1vZZYaKPgiRFh4wteNam",
   "name": "chapter_02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
