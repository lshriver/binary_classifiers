{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspaces/quantum/')\n",
    "from colors import Bcolors as bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the training data\n",
    "with open('../data/split-data/train.npy', 'rb') as f:\n",
    "    train_input = np.load(f)\n",
    "    train_labels = np.load(f)\n",
    "\n",
    "# Load the testing data\n",
    "with open('../data/split-data/test.npy', 'rb') as f:\n",
    "    test_input = np.load(f)\n",
    "    test_labels = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classifiers 🔮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random classifier 🪙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(a=None, version=2)\n",
    "\n",
    "def classify(passenger):\n",
    "    return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Death classifier 🪦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_death(item):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Runner 🏃‍♀️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(f_classify, x):\n",
    "    return list(map(f_classify, x))\n",
    "\n",
    "result = run(classify, train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation 📋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, actual) -> str:\n",
    "    correct = list(filter(\n",
    "        lambda item: item[0] == item[1],\n",
    "        list(zip(predictions, actual))\n",
    "    ))\n",
    "    \n",
    "    return '{} correct predictions out of {}. Accuracy {:.0f} %' \\\n",
    "        .format(len(correct), len(actual), 100*len(correct)/len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪙Random classifier: 377 correct predictions out of 711. Accuracy 53 %\n",
      "🪦Death classifier:  434 correct predictions out of 711. Accuracy 61 %\n"
     ]
    }
   ],
   "source": [
    "print('🪙Random classifier:', evaluate(run(classify, train_input), train_labels))\n",
    "print('🪦Death classifier: ', evaluate(run(predict_death, train_input), train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪙 Uniform confusion matrix:\n",
      " [[206 228]\n",
      " [139 138]]\n",
      "\n",
      "🪦 Death confusion matrix:\n",
      " [[434   0]\n",
      " [277   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "uniform_predictions = run(classify, train_input)\n",
    "uniform_cm = confusion_matrix(train_labels, uniform_predictions)\n",
    "print('🪙 Uniform confusion matrix:\\n', uniform_cm)\n",
    "\n",
    "print()\n",
    "\n",
    "death_predictions = run(predict_death, train_input)\n",
    "death_cm = confusion_matrix(train_labels, death_predictions)\n",
    "print('🪦 Death confusion matrix:\\n', death_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four scores from the confusion matrix\n",
    "1. Precision score\n",
    "2. Recall score\n",
    "3. Specificity\n",
    "4. Negative predictive value (NPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def specificity(matrix):\n",
    "    return matrix[0][0]/(matrix[0][0]+matrix[0][1]) if (matrix[0][0]+matrix[0][1] > 0) else 0\n",
    "\n",
    "def npv(matrix):\n",
    "    return matrix[0][0]/(matrix[0][0]+matrix[1][0]) if (matrix[0][0]+matrix[1][0] > 0) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifier_scores(train_labels, predictions, cm):\n",
    "    \n",
    "    print('The precision score of the random classifier is {:.2f}'\n",
    "        .format(precision_score(train_labels, predictions)))\n",
    "    print('The recall score of the random classifier is {:.2f}'\n",
    "        .format(recall_score(train_labels, predictions)))\n",
    "    print('The specificity score of the random classifier is {:.2f}'\n",
    "        .format(specificity(cm)))\n",
    "    print('The npv score of the random classifier is {:.2f}'\n",
    "    .   format(npv(cm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🪙 List the scores of the random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪙 \u001b[95mRandom classifier scores:\u001b[0m\n",
      "The precision score of the random classifier is 0.38\n",
      "The recall score of the random classifier is 0.50\n",
      "The specificity score of the random classifier is 0.47\n",
      "The npv score of the random classifier is 0.60\n"
     ]
    }
   ],
   "source": [
    "print(f'🪙 {bc.HEADER}Random classifier scores:{bc.ENDC}')\n",
    "print_classifier_scores(train_labels, uniform_predictions, uniform_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🪦 List the scores of the death classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪦 \u001b[95mDeath classifier scores:\u001b[0m\n",
      "The precision score of the random classifier is 0.00\n",
      "The recall score of the random classifier is 0.00\n",
      "The specificity score of the random classifier is 1.00\n",
      "The npv score of the random classifier is 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/quantum/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(f'🪦 {bc.HEADER}Death classifier scores:{bc.ENDC}')\n",
    "print_classifier_scores(train_labels, death_predictions, death_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
