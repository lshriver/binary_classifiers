{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspaces/quantum/')\n",
    "from colors import Bcolors as bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the training data\n",
    "with open('../data/split-data/train.npy', 'rb') as f:\n",
    "    train_input = np.load(f)\n",
    "    train_labels = np.load(f)\n",
    "\n",
    "# Load the testing data\n",
    "with open('../data/split-data/test.npy', 'rb') as f:\n",
    "    test_input = np.load(f)\n",
    "    test_labels = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Classifiers 🔮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random classifier 🪙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(a=None, version=2)\n",
    "\n",
    "def classify(passenger):\n",
    "    return random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Death classifier 🪦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_death(item):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Runner 🏃‍♀️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(f_classify, x):\n",
    "    return list(map(f_classify, x))\n",
    "\n",
    "result = run(classify, train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Evaluation 📋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, actual) -> str:\n",
    "    correct = list(filter(\n",
    "        lambda item: item[0] == item[1],\n",
    "        list(zip(predictions, actual))\n",
    "    ))\n",
    "    \n",
    "    return '{} correct predictions out of {}. Accuracy {:.0f} %' \\\n",
    "        .format(len(correct), len(actual), 100*len(correct)/len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪙Random classifier: 373 correct predictions out of 711. Accuracy 52 %\n",
      "🪦Death classifier:  434 correct predictions out of 711. Accuracy 61 %\n"
     ]
    }
   ],
   "source": [
    "print('🪙Random classifier:', evaluate(run(classify, train_input), train_labels))\n",
    "print('🪦Death classifier: ', evaluate(run(predict_death, train_input), train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪙 Uniform confusion matrix:\n",
      " [[204 230]\n",
      " [152 125]]\n",
      "\n",
      "🪦 Death confusion matrix:\n",
      " [[434   0]\n",
      " [277   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "uniform_predictions = run(classify, train_input)\n",
    "uniform_cm = confusion_matrix(train_labels, uniform_predictions)\n",
    "print('🪙 Uniform confusion matrix:\\n', uniform_cm)\n",
    "\n",
    "print()\n",
    "\n",
    "death_predictions = run(predict_death, train_input)\n",
    "death_cm = confusion_matrix(train_labels, death_predictions)\n",
    "print('🪦 Death confusion matrix:\\n', death_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four scores from the confusion matrix\n",
    "1. Precision score\n",
    "2. Recall score\n",
    "3. Specificity\n",
    "4. Negative predictive value (NPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def specificity(matrix):\n",
    "    return matrix[0][0]/(matrix[0][0]+matrix[0][1]) if (matrix[0][0]+matrix[0][1] > 0) else 0\n",
    "\n",
    "def npv(matrix):\n",
    "    return matrix[0][0]/(matrix[0][0]+matrix[1][0]) if (matrix[0][0]+matrix[1][0] > 0) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classifier_scores(train_labels, predictions, cm):  \n",
    "    print('The precision score of the random classifier is {:.2f}'\n",
    "        .format(precision_score(train_labels, predictions)))\n",
    "    print('The recall score of the random classifier is {:.2f}'\n",
    "        .format(recall_score(train_labels, predictions)))\n",
    "    print('The specificity score of the random classifier is {:.2f}'\n",
    "        .format(specificity(cm)))\n",
    "    print('The npv score of the random classifier is {:.2f}'\n",
    "    .   format(npv(cm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🪙 List the scores of the random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪙 \u001b[95mRandom classifier scores:\u001b[0m\n",
      "The precision score of the random classifier is 0.35\n",
      "The recall score of the random classifier is 0.45\n",
      "The specificity score of the random classifier is 0.47\n",
      "The npv score of the random classifier is 0.57\n",
      "\n",
      "🪙 \u001b[95mRandom classifier confusion matrix:\u001b[0m\n",
      " [[204 230]\n",
      " [152 125]]\n"
     ]
    }
   ],
   "source": [
    "print(f'🪙 {bc.HEADER}Random classifier scores:{bc.ENDC}')\n",
    "print_classifier_scores(train_labels, uniform_predictions, uniform_cm)\n",
    "print()\n",
    "print(f'🪙 {bc.HEADER}Random classifier confusion matrix:{bc.ENDC}\\n', uniform_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🪦 List the scores of the death classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🪦 \u001b[95mDeath classifier scores:\u001b[0m\n",
      "The precision score of the random classifier is 0.00\n",
      "The recall score of the random classifier is 0.00\n",
      "The specificity score of the random classifier is 1.00\n",
      "The npv score of the random classifier is 0.61\n",
      "\n",
      "🪦 \u001b[95mDeath classifier confusion matrix:\u001b[0m\n",
      " [[434   0]\n",
      " [277   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/quantum/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(f'🪦 {bc.HEADER}Death classifier scores:{bc.ENDC}')\n",
    "print_classifier_scores(train_labels, death_predictions, death_cm)\n",
    "print()\n",
    "print(f'🪦 {bc.HEADER}Death classifier confusion matrix:{bc.ENDC}\\n', death_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 😈 List the scores of the hypocrite classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a hypocrite classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing A hypocrite classifier\n",
    "def hypocrite(passenger, weight):\n",
    "    return round(min(1,max(0,weight*0.5+random.uniform(0, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😈 \u001b[95mHypocrite classifier scores:\u001b[0m\n",
      "The precision score of the random classifier is 0.36\n",
      "The recall score of the random classifier is 0.25\n",
      "The specificity score of the random classifier is 0.72\n",
      "The npv score of the random classifier is 0.60\n",
      "\n",
      "😈 \u001b[95mHypocrite classifier confusion matrix:\u001b[0m\n",
      " [[311 123]\n",
      " [208  69]]\n"
     ]
    }
   ],
   "source": [
    "# Listing The scores of the hypocrite classifier\n",
    "hypocrite_predictions = run(lambda passenger: hypocrite(passenger, -0.5), train_input)\n",
    "hypocrite_cm = confusion_matrix(train_labels, hypocrite_predictions)\n",
    "\n",
    "print(f'😈 {bc.HEADER}Hypocrite classifier scores:{bc.ENDC}')\n",
    "print_classifier_scores(train_labels, hypocrite_predictions, hypocrite_cm)\n",
    "print()\n",
    "print(f'😈 {bc.HEADER}Hypocrite classifier confusion matrix:{bc.ENDC}\\n', hypocrite_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
